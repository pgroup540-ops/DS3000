# Hallucination Reduction Pipeline

A comprehensive two-stage training pipeline to reduce hallucinations in medical text generation using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Cross-Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Mac%20%7C%20Linux-lightgrey.svg)](https://github.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ¯ Overview

This pipeline trains large language models to generate factually accurate medical summaries by:

1. **Stage A (SFT)**: Teaching medical knowledge and domain expertise
2. **Stage B (DPO)**: Teaching preference for truth over hallucinations

**Key Features:**
- âœ… Cross-platform (Windows, Mac, Linux)
- âœ… Memory-efficient (LoRA fine-tuning)
- âœ… Adversarial data generation
- âœ… Evidence-based training
- âœ… Comprehensive evaluation

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Complete Workflow](#-complete-workflow)
- [Project Structure](#-project-structure)
- [Documentation](#-documentation)
- [Hardware Requirements](#-hardware-requirements)
- [Troubleshooting](#-troubleshooting)

---

## ğŸš€ Quick Start

### For Windows Users

See **[WINDOWS_QUICKSTART.md](WINDOWS_QUICKSTART.md)** for detailed Windows-specific instructions.

### For Mac/Linux Users

```bash
# 1. Clone repository
git clone https://github.com/YOUR_USERNAME/Reduction-of-Hallucinations.git
cd Reduction-of-Hallucinations

# 2. Install dependencies
pip install pandas scikit-learn torch transformers peft tqdm openpyxl

# 3. Setup directories
python setup_directories.py

# 4. Run complete pipeline (after preparing data)
python preprocess_data.py --generate-adversarial
python generate_phase2_data.py
python stage_a_sft_training.py
python stage_b_dpo_training.py
```

---

## ğŸ’» Installation

### Prerequisites

- **Python:** 3.8 or higher
- **RAM:** 16GB minimum (32GB recommended)
- **Storage:** 50GB free space
- **GPU (Optional):** NVIDIA GPU with 16GB+ VRAM

### Install Dependencies

```bash
pip install pandas scikit-learn torch transformers peft tqdm openpyxl
```

Or use the requirements file:
```bash
pip install -r requirements.txt
```

### Setup Project Structure

```bash
python setup_directories.py
```

This creates all necessary directories automatically.

---

## ğŸ“Š Complete Workflow

### Phase 0: Setup

```bash
# Create directory structure
python setup_directories.py

# Verify installation
python -c "import torch; import transformers; import peft; print('âœ“ All imports OK')"
```

### Phase 1: Data Preparation & SFT Training

#### Step 1: Prepare Raw Data

Place your data in `Sets/` with these columns:
- `id` - Unique identifier
- `clinical_note` - Clinical text
- `model_summary` - Summary
- `label` - "factual" or "hallucinated"

#### Step 2: Preprocess Data

```bash
python preprocess_data.py \
    --input-dir "Sets" \
    --output-dir "phase1_data/sft" \
    --generate-adversarial \
    --adversarial-ratio 0.5
```

**Options:**
- `--normalize` - Text normalization
- `--redact-phi` - PHI redaction (HIPAA compliance)
- `--generate-adversarial` - Generate hard negatives
- `--generate-evidence` - Add evidence annotations

#### Step 3: Train Stage A (SFT)

```bash
python stage_a_sft_training.py \
    --model_name "meta-llama/Llama-2-7b-hf" \
    --train_data_path "phase1_data/sft/train_set_processed.csv" \
    --val_data_path "phase1_data/sft/validation_set_processed.csv" \
    --num_epochs 2 \
    --learning_rate 2e-4 \
    --batch_size 8 \
    --output_dir "models/sft_specialist"
```

**Expected Time:** 2-4 hours on GPU

### Phase 2: DPO Training

#### Step 4: Generate DPO Data

```bash
python generate_phase2_data.py \
    --phase1_dir "phase1_data/sft" \
    --phase2_dir "phase2_data/dpo" \
    --adversarial_ratio 1.0
```

This automatically creates triplet data (prompt, chosen, rejected).

#### Step 5: Train Stage B (DPO)

```bash
python stage_b_dpo_training.py \
    --sft_model_path "models/sft_specialist/final_model" \
    --train_data_path "phase2_data/dpo/train_dpo.jsonl" \
    --val_data_path "phase2_data/dpo/val_dpo.jsonl" \
    --num_epochs 2 \
    --learning_rate 5e-6 \
    --beta 0.1 \
    --batch_size 4 \
    --output_dir "models/dpo_hallucination_resistant"
```

**Expected Time:** 2-4 hours on GPU

**Key Difference:** Learning rate is 100x lower than SFT!

### Phase 3: Inference & Evaluation

```bash
python sft_inference.py \
    --model_path "models/dpo_hallucination_resistant/final_model" \
    --clinical_note "Patient reports fever of 38.5Â°C and cough for 3 days..."
```

---

## ğŸ“ Project Structure

```
Reduction-of-Hallucinations/
â”œâ”€â”€ Sets/                               # Raw data
â”œâ”€â”€ phase1_data/sft/                    # Phase 1 processed data
â”œâ”€â”€ phase2_data/dpo/                    # Phase 2 DPO triplets
â”œâ”€â”€ models/                             # Trained models
â”‚   â”œâ”€â”€ sft_specialist/
â”‚   â””â”€â”€ dpo_hallucination_resistant/
â”‚
â”œâ”€â”€ Core Scripts:
â”œâ”€â”€ setup_directories.py                # Setup project structure
â”œâ”€â”€ preprocess_data.py                  # Phase 1 preprocessing
â”œâ”€â”€ generate_phase2_data.py             # Phase 1 â†’ Phase 2 bridge
â”œâ”€â”€ stage_a_sft_training.py             # Stage A training
â”œâ”€â”€ stage_b_dpo_training.py             # Stage B training
â”œâ”€â”€ sft_inference.py                    # Model inference
â”‚
â”œâ”€â”€ Data Processing Modules:
â”œâ”€â”€ text_normalizer.py                  # Text normalization
â”œâ”€â”€ phi_redactor.py                     # PHI redaction
â”œâ”€â”€ adversarial_augmenter.py            # Hard negative generation
â”œâ”€â”€ evidence_annotator.py               # Evidence annotation
â”œâ”€â”€ dpo_triplet_generator.py            # DPO triplet creation
â”œâ”€â”€ data_splitter.py                    # Data splitting
â”‚
â”œâ”€â”€ Dataset Loaders:
â”œâ”€â”€ sft_dataset.py                      # SFT dataset
â”œâ”€â”€ dpo_dataset.py                      # DPO dataset
â”‚
â”œâ”€â”€ Documentation:
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ WINDOWS_QUICKSTART.md               # Windows guide
â””â”€â”€ Guides/                             # Detailed guides
    â”œâ”€â”€ STAGE_A_SFT_GUIDE.md
    â”œâ”€â”€ STAGE_B_DPO_GUIDE.md
    â””â”€â”€ WINDOWS_SETUP.md
```

---

## ğŸ“š Documentation

### Quick Start Guides
- **[WINDOWS_QUICKSTART.md](WINDOWS_QUICKSTART.md)** - Windows-specific setup
- **[Guides/STAGE_A_QUICK_START.md](Guides/STAGE_A_QUICK_START.md)** - SFT quick start
- **[Guides/STAGE_B_QUICK_START.txt](Guides/STAGE_B_QUICK_START.txt)** - DPO quick start

### Comprehensive Guides
- **[Guides/STAGE_A_SFT_GUIDE.md](Guides/STAGE_A_SFT_GUIDE.md)** - Complete SFT guide
- **[Guides/STAGE_B_DPO_GUIDE.md](Guides/STAGE_B_DPO_GUIDE.md)** - Complete DPO guide
- **[Guides/NEXT_STEPS_EXECUTION_GUIDE.md](Guides/NEXT_STEPS_EXECUTION_GUIDE.md)** - Next steps

---

## ğŸ’¾ Hardware Requirements

### Minimum (CPU Only)
- **CPU:** Modern multi-core processor
- **RAM:** 16GB
- **Storage:** 50GB free
- **Training Time:** 10-20 hours per stage

### Recommended (GPU)
- **GPU:** NVIDIA RTX 3090/4090 (24GB VRAM)
- **RAM:** 32GB
- **Storage:** 100GB free
- **Training Time:** 2-4 hours per stage

### Cloud Options
- **Google Colab:** Free Tesla T4 (15GB)
- **AWS EC2:** p3.2xlarge with V100 (16GB)
- **Azure ML:** NC-series VMs

---

## ğŸ”§ Key Parameters

### Stage A (SFT)
| Parameter | Default | Description |
|-----------|---------|-------------|
| `learning_rate` | 2e-4 | Standard LoRA rate |
| `batch_size` | 8 | Adjust for GPU memory |
| `num_epochs` | 2 | 1-3 recommended |
| `lora_r` | 16 | LoRA rank |
| `max_length` | 512 | Sequence length |

### Stage B (DPO)
| Parameter | Default | Description |
|-----------|---------|-------------|
| `learning_rate` | 5e-6 | **100x lower than SFT!** |
| `batch_size` | 4 | Smaller (dual model) |
| `beta` | 0.1 | KL penalty weight |
| `num_epochs` | 2 | 1-3 recommended |

---

## ğŸ› Troubleshooting

### Common Issues

**"CUDA out of memory"**
```bash
# Reduce batch size
python stage_a_sft_training.py --batch_size 4
python stage_b_dpo_training.py --batch_size 2
```

**"Module not found"**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**"Phase 2 data not found"**
```bash
# Run the bridge script first
python generate_phase2_data.py
```

**Windows path issues**
- Use forward slashes: `"phase1_data/sft"`
- Or double backslashes: `"phase1_data\\sft"`

### Platform-Specific Guides
- **Windows:** See [WINDOWS_QUICKSTART.md](WINDOWS_QUICKSTART.md)
- **Mac/Linux:** Standard commands work as-is

---

## ğŸ“ˆ Expected Results

### After Stage A (SFT)
- Model learns medical knowledge âœ“
- Generates coherent summaries âœ“
- Hallucination rate: ~30-40%

### After Stage B (DPO)
- Model prefers factual responses âœ“
- Hallucination rate: ~5-15% âœ“
- Maintains medical knowledge âœ“

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

## ğŸ“„ License

[Your License Here]

---

## ğŸ™ Acknowledgments

- **DPO Paper:** [Rafailov et al., 2023](https://arxiv.org/abs/2305.18290)
- **LoRA:** [Hu et al., 2021](https://arxiv.org/abs/2106.09714)
- **Transformers:** Hugging Face Team
- **PEFT:** Hugging Face PEFT library

---

## ğŸ“§ Contact

For questions or issues:
- Open an issue on GitHub
- Check existing documentation in `Guides/`

---

## âš¡ Quick Commands Reference

```bash
# Setup
python setup_directories.py

# Phase 1
python preprocess_data.py --generate-adversarial
python stage_a_sft_training.py --num_epochs 2

# Phase 2
python generate_phase2_data.py
python stage_b_dpo_training.py --learning_rate 5e-6

# Inference
python sft_inference.py --model_path "models/dpo_hallucination_resistant/final_model"
```

---

**Status:** âœ… Fully functional and cross-platform compatible

**Last Updated:** November 2024
