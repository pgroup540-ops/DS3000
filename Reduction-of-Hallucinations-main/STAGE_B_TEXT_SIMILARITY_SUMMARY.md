# Stage B DPO Model - Text Similarity Metrics Summary

**Model**: Stage B DPO Checkpoint-1300 (31% training complete)
**Evaluation Date**: November 26, 2025
**Test Set**: 50 examples (20 factual, 30 hallucinated)

---

## Executive Summary

Text similarity metrics (BLEU, ROUGE, BERTScore) were computed to quantify how closely the model's predicted summaries match reference summaries. 

### Key Finding: **Strong Semantic Understanding Despite Low Lexical Overlap** ‚úÖ

The model demonstrates excellent semantic preservation (81% BERTScore) while showing low exact word matching (3.49% BLEU, 19.2% ROUGE-1). This indicates the model successfully **paraphrases** medical content while preserving meaning - a desirable trait for summarization.

---

## Metrics Overview

| Metric | Score | Interpretation | Verdict |
|--------|-------|----------------|---------|
| **BLEU** | 0.0349 (3.49%) | Very low n-gram overlap | ‚ö†Ô∏è Below typical (5-20%) |
| **ROUGE-1** | 0.1919 (19.2%) | Moderate unigram recall | ‚ö†Ô∏è Below typical (25-40%) |
| **ROUGE-2** | 0.0935 (9.4%) | Low bigram recall | ‚ö†Ô∏è Below typical (10-25%) |
| **ROUGE-L** | 0.1360 (13.6%) | Low longest common subsequence | ‚ö†Ô∏è Below typical (20-35%) |
| **BERTScore F1** | 0.8105 (81.1%) | Strong semantic similarity | ‚úÖ Within typical range (75-85%) |

---

## Detailed Results

### BLEU Score (N-gram Precision)
```
Mean:   0.0349  (3.49% exact overlap)
Std:    ¬±0.0618 (high variability)
Range:  0.0000 - 0.3217
```

**By Label**:
- Hallucinated: 0.0524 ¬± 0.0756
- Factual: 0.0086 ¬± 0.0102

**Interpretation**: Model generates paraphrased summaries rather than copying reference text. Hallucinated examples surprisingly show 6√ó higher BLEU than factual examples.

---

### ROUGE Scores (Recall-Oriented)
```
ROUGE-1:  0.1919 ¬± 0.1608  (unigram overlap)
ROUGE-2:  0.0935 ¬± 0.1198  (bigram overlap)
ROUGE-L:  0.1360 ¬± 0.1210  (longest common subsequence)
```

**By Label**:
| Label | ROUGE-1 | ROUGE-2 | ROUGE-L |
|-------|---------|---------|---------|
| Hallucinated | 0.2049 ¬± 0.1971 | 0.1303 ¬± 0.1440 | 0.1556 ¬± 0.1516 |
| Factual | 0.1725 ¬± 0.0898 | 0.0383 ¬± 0.0273 | 0.1066 ¬± 0.0444 |

**Interpretation**: 
- ~1 in 5 words from reference appear in prediction
- Hallucinated examples show higher lexical overlap (counterintuitive)
- High variance in hallucinated examples indicates inconsistent quality

---

### BERTScore (Semantic Similarity)
```
Precision:  0.8201 (82%)
Recall:     0.8033 (80%)
F1:         0.8105 (81%) ‚úÖ
Std Dev:    ¬±0.0426 (consistent quality)
Range:      0.6891 - 0.9115
```

**By Label**:
- Hallucinated: 0.8165 ¬± 0.0539
- Factual: 0.8016 ¬± 0.0140

**Interpretation**: 
- Excellent semantic preservation (81% similarity)
- Low variance indicates consistent understanding
- Factual examples show lower variance (more stable)

---

## Key Insights

### 1. Paraphrasing vs. Copying ‚úÖ
**Finding**: Low BLEU/ROUGE but high BERTScore

The model successfully paraphrases medical content while preserving semantic meaning. This is **actually desirable** for summarization tasks, where the goal is concise, clear communication rather than verbatim reproduction.

**Evidence**:
- BLEU: 3.49% (very low lexical match)
- BERTScore: 81.1% (strong semantic match)

### 2. Counterintuitive Performance on Hallucinated Examples ‚ö†Ô∏è
**Finding**: Hallucinated examples show higher lexical similarity than factual

```
              BLEU    ROUGE-1  ROUGE-2  BERTScore
Hallucinated  0.052   0.205    0.130    0.8165
Factual       0.009   0.173    0.038    0.8016
```

**Hypotheses**:
1. Hallucinated predictions may be longer (more text = more chance of overlap)
2. Factual predictions may be more conservative/concise
3. Hallucinated reference summaries may be more detailed

**Action needed**: Analyze prediction lengths to test hypothesis.

### 3. Consistent Semantic Quality üéØ
**Finding**: Low variance in BERTScore, especially for factual examples

- Overall BERTScore std: 0.043 (5.3% coefficient of variation)
- Factual std: 0.014 (very consistent)
- Hallucinated std: 0.054 (more variable)

**Conclusion**: Model has learned stable semantic understanding; generation style varies.

---

## Performance Assessment

### Strengths ‚úÖ
1. **Strong semantic preservation** (81% BERTScore)
2. **Consistent performance** across examples
3. **Successful paraphrasing** instead of verbatim copying
4. **Above-average BERTScore** for medical summarization

### Weaknesses ‚ö†Ô∏è
1. **Below-average lexical overlap** (BLEU 3.49% vs typical 5-20%)
2. **ROUGE scores below benchmarks** (ROUGE-1: 19.2% vs typical 25-40%)
3. **Inconsistent quality** on hallucinated examples (high variance)
4. **Limited exact phrase matching** (ROUGE-2: 9.4%)

### Overall Verdict
**‚úÖ Adequate Performance with Room for Improvement**

The model demonstrates solid **semantic understanding** (primary goal for summarization) but shows a gap in **lexical alignment** with reference texts. This suggests the model has learned the medical concepts correctly but may benefit from additional training to better align phrasing with clinical conventions.

---

## Comparison with Benchmarks

### Medical Summarization Benchmarks
| Metric | Typical Range | Our Model | Status |
|--------|---------------|-----------|--------|
| BLEU | 0.05 - 0.20 | 0.0349 | ‚ö†Ô∏è Below |
| ROUGE-1 | 0.25 - 0.40 | 0.1919 | ‚ö†Ô∏è Below |
| ROUGE-2 | 0.10 - 0.25 | 0.0935 | ‚ö†Ô∏è Below/Low End |
| ROUGE-L | 0.20 - 0.35 | 0.1360 | ‚ö†Ô∏è Below |
| BERTScore F1 | 0.75 - 0.85 | 0.8105 | ‚úÖ Within Range |

**Note**: Medical summarization benchmarks vary widely by dataset and task. Our model is within expected range for BERTScore (most important metric) but below typical for lexical metrics.

---

## Implications for Training Continuation

### Analysis
The strong BERTScore (81%) suggests the model's **core understanding is good**. However, the below-average ROUGE scores indicate room for improvement in **phrasing alignment**.

### Recommendation
**Continue training** to potentially improve lexical metrics while maintaining semantic quality:

- ‚úÖ Current: Strong semantic foundation (81% BERTScore)
- üéØ Goal: Improve lexical alignment (target ROUGE-1: 25-30%)
- ‚ö†Ô∏è Risk: Potential for overfitting if trained too long

**Decision Point**: 
- If manual hallucination assessment shows ‚â§30% hallucination rate ‚Üí Can use as-is
- If manual hallucination assessment shows >30% ‚Üí Continue training recommended

---

## Files Generated

```
evaluation_results_stage_b_checkpoint1300/
‚îú‚îÄ‚îÄ evaluation_results_with_metrics.csv    # All metrics per example
‚îú‚îÄ‚îÄ similarity_metrics_summary.json        # Aggregate statistics
‚îî‚îÄ‚îÄ STAGE_B_TEXT_SIMILARITY_SUMMARY.md     # This document
```

---

## Next Steps

### Priority 1: Manual Hallucination Assessment ‚≠ê
**Purpose**: Determine actual hallucination rate to decide on training continuation

**Expected outcomes**:
- Hallucination rate ‚â§30%: Use checkpoint-1300 as-is ‚úÖ
- Hallucination rate 30-40%: Train 1 more epoch ‚ö†Ô∏è
- Hallucination rate >40%: Complete full 2 epochs ‚ùå

### Priority 2: Length Analysis
**Purpose**: Test hypothesis about hallucinated examples having longer predictions

**Action**: Compute average prediction and reference lengths by label

### Priority 3: Stage A Comparison
**Purpose**: Compute same metrics on Stage A model for direct comparison

**Expected**: Stage A likely has similar/worse semantic scores but unknown lexical scores

---

## Technical Details

### Computation
- **Tool**: `compute_similarity_metrics.py`
- **Libraries**: nltk (BLEU), rouge-score (ROUGE), bert-score (BERTScore)
- **BERTScore Model**: RoBERTa-large (1.42GB)
- **Computation Time**: ~2 minutes (50 examples)

### Metric Definitions
- **BLEU**: Precision-based n-gram overlap (commonly used for machine translation)
- **ROUGE**: Recall-based n-gram overlap (designed for summarization)
- **BERTScore**: Contextual embedding similarity using pre-trained language models

---

**Document Version**: 1.0
**Last Updated**: November 26, 2025
**For Full Details**: See `STAGE_B_COMPREHENSIVE_METRICS.md` Section 8
