# Stage A SFT - Quick Start Reference\n\n## Files Created\n\n```\n✓ sft_dataset.py              - Data loading & preprocessing\n✓ stage_a_sft_training.py     - Main training script with LoRA\n✓ sft_inference.py            - Model inference & generation\n✓ requirements_training.txt   - Dependencies\n✓ STAGE_A_SFT_GUIDE.md        - Complete documentation\n```\n\n## 60-Second Setup\n\n```bash\n# 1. Install dependencies\npip install -r requirements_training.txt\n\n# 2. Run training (2 epochs, ~1-2 hours on GPU)\npython stage_a_sft_training.py \\\n    --model_name \"meta-llama/Llama-2-7b-hf\" \\\n    --num_epochs 2 \\\n    --learning_rate 2e-4\n\n# 3. Test inference\npython sft_inference.py \\\n    --model_path ./models/sft_specialist/final_model \\\n    --clinical_note \"Patient has fever and cough. Tested positive for influenza.\"\n```\n\n## Key Parameters Explained\n\n| Parameter | Value | Purpose |\n|-----------|-------|----------|\n| `num_epochs` | 2 | Training passes (1-3 recommended) |\n| `learning_rate` | 2e-4 | Update step size (standard LoRA) |\n| `batch_size` | 8 | Examples per update (reduce if OOM) |\n| `lora_r` | 16 | Adapter capacity (32 if underfitting) |\n| `max_length` | 512 | Token limit per example |\n\n## What to Expect\n\n### Training Output\n```\nEpoch 1/2: Train Loss 2.14, Val Loss 2.09\nEpoch 2/2: Train Loss 1.82, Val Loss 1.80\nTraining completed!\nSaved to ./models/sft_specialist/final_model\n```\n\n### Model Output Example\n```\nInput:  Patient reports fever of 38.5°C and cough.\nOutput: The patient has fever and cough.\n```\n\n## Troubleshooting\n\n### \"CUDA out of memory\"\n```bash\npython stage_a_sft_training.py --batch_size 4 --lora_r 8\n```\n\n### \"Loss not decreasing\"\n```bash\npython stage_a_sft_training.py --num_epochs 3 --lora_r 32\n```\n\n### \"Model won't load for inference\"\n```bash\npython sft_inference.py --model_path ... --device cpu\n```\n\n## Model Architecture\n\n```\nBase Model (Llama-2-7b)\n        ↓\n    LoRA Adapter (r=16)\n        ↓\n   Fine-tuned Specialist\n        ↓\n   Next Token Prediction\n   (Cross-Entropy Loss)\n```\n\n## Training Data Format\n\n**Input CSV**: `phase1_data/sft/train_set_processed.csv`\n\n```csv\nid,clinical_note,model_summary,label,hallucination_type\n1,Patient had fever. Tested positive for influenza A.,The patient tested positive for influenza A.,factual,\n2,Patient recovering from COVID-19. No respiratory distress.,The patient is recovering from COVID-19.,factual,\n```\n\n**Key**: Only rows with `label='factual'` are used for training.\n\n## Output Structure\n\n```\nmodels/sft_specialist/\n├── final_model/                  # Use this for inference\n│   ├── adapter_config.json      # LoRA configuration\n│   ├── adapter_model.bin        # Trained weights\n│   └── tokenizer.json\n├── checkpoint_epoch_1/          # Intermediate saves\n├── checkpoint_epoch_2/\n└── training_stats.json          # Loss metrics\n```\n\n## Performance Characteristics\n\n| Hardware | Batch Size | Time/Epoch | Memory |\n|----------|-----------|-----------|--------|\n| A100 GPU | 32 | 10-15 min | 40GB |\n| V100 GPU | 8 | 20-30 min | 32GB |\n| A10 GPU | 4 | 30-45 min | 24GB |\n| M1/M2 Mac | 2 | 60-90 min | 16GB |\n\n## Common Commands\n\n### Training Variants\n\n```bash\n# Fast (1 epoch)\npython stage_a_sft_training.py --num_epochs 1 --batch_size 16\n\n# Standard (2 epochs, recommended)\npython stage_a_sft_training.py --num_epochs 2 --batch_size 8\n\n# Thorough (3 epochs, large model)\npython stage_a_sft_training.py --num_epochs 3 --lora_r 32 --batch_size 4\n\n# Memory-efficient (Mac/CPU)\npython stage_a_sft_training.py --batch_size 2 --lora_r 8 --device cpu\n```\n\n### Inference Variants\n\n```bash\n# Single example\npython sft_inference.py --model_path ... --clinical_note \"...\"\n\n# Batch from file\npython sft_inference.py --model_path ... --input_file notes.txt\n\n# Interactive\npython sft_inference.py --model_path ...\n```\n\n## When to Move to Stage B\n\n✓ Training loss converges (decreases smoothly)\n✓ Model generates coherent medical text\n✓ Output format matches your templates\n✓ No obvious hallucinations yet\n\n→ Then proceed to Stage B: Direct Preference Optimization (DPO)\n\n## Advanced: Using Different Base Models\n\n```bash\n# Llama-3 (8B)\npython stage_a_sft_training.py --model_name \"meta-llama/Meta-Llama-3-8B\"\n\n# Mistral (7B, faster)\npython stage_a_sft_training.py --model_name \"mistralai/Mistral-7B\"\n\n# Smaller model (faster training, lower quality)\npython stage_a_sft_training.py --model_name \"meta-llama/Llama-2-7b\" --lora_r 8\n```\n\n## Key Statistics\n\n- **Total training examples**: ~50-100 factual pairs (from phase1_data)\n- **Validation examples**: ~10-20\n- **Total parameters**: 7B (for Llama-2-7b)\n- **Trainable parameters**: ~0.3B (with LoRA r=16)\n- **Training time**: 1-3 hours (GPU) / 3-6 hours (CPU)\n\n## Next Steps\n\n1. After training completes → model saved to `./models/sft_specialist/final_model`\n2. Test inference to verify medical expertise\n3. Evaluate on held-out test set\n4. Prepare for Stage B: DPO (hard negatives)\n\n## Links\n\n- Full Guide: `STAGE_A_SFT_GUIDE.md`\n- LoRA Paper: https://arxiv.org/abs/2106.09714\n- PEFT Docs: https://huggingface.co/docs/peft\n"