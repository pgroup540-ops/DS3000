{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd",
    "data = [",
    "    {",
    "        \"Model Name\": \"Llama 3.1 8B\",",
    "        \"Developer\": \"Meta\",",
    "        \"Parameter Count\": \"8 Billion\",",
    "        \"Context Window\": \"128k tokens\",",
    "        \"Key Features\": \"Multilingual (8+ languages), Tool Use, Long Context, 15T training tokens\",",
    "        \"Description\": \"A dense transformer model designed for efficiency and long-context reasoning. It excels in multilingual dialogue and agentic workflows, bridging the gap between edge-device latency and server-grade performance.\"",
    "    },",
    "    {",
    "        \"Model Name\": \"Gemma 2 9B\",",
    "        \"Developer\": \"Google\",",
    "        \"Parameter Count\": \"9 Billion\",",
    "        \"Context Window\": \"8k tokens\",",
    "        \"Key Features\": \"Knowledge Distillation, Sliding Window Attention, Soft-capping, High performance-to-size ratio\",",
    "        \"Description\": \"Built on the same research and technology as Gemini. It utilizes knowledge distillation from larger models to achieve outsized performance for its weight class, particularly in reasoning and coding tasks.\"",
    "    },",
    "    {",
    "        \"Model Name\": \"Mistral 7B (v0.3)\",",
    "        \"Developer\": \"Mistral AI\",",
    "        \"Parameter Count\": \"7.3 Billion\",",
    "        \"Context Window\": \"32k tokens\",",
    "        \"Key Features\": \"Sliding Window Attention (SWA), Grouped-Query Attention (GQA), Rolling Buffer Cache\",",
    "        \"Description\": \"A highly efficient model known for its speed and low memory footprint. Its architectural innovations (SWA/GQA) allow it to handle longer sequences with lower compute costs, making it a favorite for local deployment.\"",
    "    },",
    "    {",
    "        \"Model Name\": \"Qwen 2.5 7B\",",
    "        \"Developer\": \"Alibaba Cloud\",",
    "        \"Parameter Count\": \"7.61 Billion\",",
    "        \"Context Window\": \"128k tokens\",",
    "        \"Key Features\": \"Multilingual (29+ languages), Structured Output (JSON), Coding & Math specialization\",",
    "        \"Description\": \"A powerhouse for coding and mathematics that supports over 29 languages. It is specifically optimized to follow complex instructions and generate structured data formats like JSON, making it ideal for enterprise applications.\"",
    "    },",
    "    {",
    "        \"Model Name\": \"Phi-3 Mini\",",
    "        \"Developer\": \"Microsoft\",",
    "        \"Parameter Count\": \"3.8 Billion\",",
    "        \"Context Window\": \"128k tokens\",",
    "        \"Key Features\": \"High-quality Synthetic Data Training, Mobile Deployment, Reasoning-dense\",",
    "        \"Description\": \"A lightweight model trained on a 'textbook-quality' dataset of 3.3 trillion tokens. Despite its small size (deployable on phones), it rivals larger models in logic and reasoning benchmarks.\"",
    "    }",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel",
    "output_file = \"llm_models_comparison.xlsx\"",
    "df.to_excel(output_file, index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Successfully created {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}